# Rule-based metamorphic testing

## Introduction

This repository is for the paper [Rule-based metamorphic testing for autonomous driving model]().

We propose a declarative, rule-based metamorphic testing framework called **RMT**. It is the first to expand automated testing capability for autonomous vehicles by enabling easy mapping of traffic regulations to executable metamorphic relations and to demonstrate the benefits of *expressivity*, *customization*, and *pluggability*.

![RMT overview](asset/rmt_overview_2.png)

It provides three components that work in concert: (1) a **domain-specific language** that enables an expert to express higher-order, compositional metamorphic relations, (2) **pluggable transformation engines** built on a variety of image and graphics processing techniques, and (3) **automated test generation** that translates a human-written rule to a corresponding executable, metamorphic relation and synthesizes meaningful inputs. 

 <!-- While existing techniques such as [DeepTest]() and [DeepRoad]() hardcode metamorphic relations (MRs), RMT enables domain experts to specify custom rules using a domain-specific language. RMT then translates each rule to a corresponding MR. It then injects meaningful input transformation entailed by the custom rule by leveraging pluggable input transformation engines.  -->

RMT currently incorporates three kinds of graphics and image transformation engines: 
1. object insertion based on semantic labels, 
2. affine transformations in Open CV, 
3. image-to-image translation based on GANs. 

Such pluggable transformation engines can be supplied by domain experts to express and test complex traffic rules, such as simultaneously changing the driving time to night time and injecting a pedestrian in front of a vehicle.

In this paper, we express 7 Simple Rules (SR): 
- **Rule 1**: Adding a vehicle *x* meters in front of the main vehicle should cause a deceleration to less than *t*
- **Rule 2**: Adding a speed limit sign at the roadside will cause a deceleration to less than *t*.
- **Rule 3-5**: Adding a vehicle/bicycle/pedestrian *x* meters in front of the main vehicle will cause a deceleration by *t<sub>1</sub>%* to *t<sub>2</sub>%*.
- **Rule 6-7**: Changing the driving scenes to night time or rainy day will cause a deceleration by *t<sub>1</sub>%* to *t<sub>2</sub>%*.

and 2 Composite Rules(CR):
- **Rule 8**: Adding a vehicle *x* meters in front of the main vehicle and changing to rainy day will cause a deceleration by *t<sub>1</sub>%* to *t<sub>2</sub>%*.
- **Rule 9**: Compared with adding a vehicle *x<sub>1</sub>* meters in front of the main vehicle, adding a vehicle *x<sub>2</sub>* meters (*x<sub>2</sub> < x<sub>1</sub>*) in front of the main vehicle will cause a bigger deceleration.

*For detailed introductions to each rules, please refer to the paper.*

<!-- *Rules 1* and *2*  are derived from traffic laws that give specific oracles (i.e., speed limits). Because they use a single transformation, *X<sub>N1</sub>* is always set to *X<sub>O</sub>*. *Rule 1* tests whether a driving model follows a safe driving distance regulation, when a vehicle appears in the front and close to the main vehicle. This is based on NSW Australia's traffic law (Figure XXX). To keep *3* seconds response time, there should be *50* meter distance from the front vehicle when the speed is above 60 km/h. Therefore, *M(X<sub>O</sub>)* should be no more than 60 km/h when a vehicle is located in the front less than 50 meters away from the main vehicle. *Rule 2* tests scenarios of seeing a speed limit sign. 

*Rules 3* to *7* are designed based on traffic rules that specify required driving behavior in certain circumstances without specific speed oracles. For example, Texas traffic law requires drivers to ''slow down and increase the following distance when the road is wet''. Since the speed oracle is not specified, we quantify the required driving behavior as the speed decrease ratio in a range. *Rules 3*, *4* and *5* test anomalous scenarios where objects suddenly appear in the front and close to the main vehicle, which is similar to *Rule 1* but allow the injection of more types of objects. *Rules 6* and *7* test the impact of different driving scenes on speed.

*Rules 8* and *9* are composite rules that compare the outcome of more than one transformation. Compared with the prior work that test relations between the original input and its modified input, **RMT** makes it easier to test the *compounding*, *proportional effect* of more than one transformation. -->


<!-- Rules 1 - 5 are for testing speed prediction models. Rule 6 is to test steering angle prediction models. While Rules 1-5 are inequality metamorphic relations, Rule 6 is actually an equality metamorphic relation, created to check how partial disappearance of lanes affects steering angle prediction. 

In RMT, more diverse rules can be generated by combining existing rules to cover more complex real-world scenarios. For example, Rule 1 can be combined with Rule 5 to generate images that add a vehicle in front of the main vehicle on a rainy day. In Section 3, we combine Rules 1-3 with Rule 5 to generate three more rules (Rules 7-9) to evaluate the benefits brought by such composite rules:
7. Changing to rainy & Adding a vehicle in the front
8. Changing to rainy & Adding a bicycle in the front
9. Changing to rainy & Adding a pedestrian in the front -->

## Project architecture

> `train.py`: train driving models

> `mt_checker.py`: generate metamorphic test result

> `human_evaluation.py`: perform the human evaluation

> `rmt.xml`: the config of each rule

> **source_datasets**: folders for source images

> **follow_up_datasets**: folders for transformed images

> **models**: folders for checkpoints of driving models and generators

> **generators**: folders for image transformation generators

>> `rain.py`: implement transformations of ***Rule 5***

>>Pix2pixHD-master /`image_control.py`: implement transformations of ***Rule 1-3***

>>UNIT / `test_batch.py`: implement transformations of ***Rule 4***

>>UGATIT / `main.py`: implement transformations of ***Rule 5***

>>SCNN / `gen.py`: implement transformations of ***Rule 6***

> **UI**：Folder for Ui Files
>> `SimpleGUI.py`: The UI of RMT

## Prerequisites

+ Python version: 3.6
+ GPU : NVIDIA P100 12G memory (Requiring 12G memory or larger for Pix2pixHD)
+ Pytorch version: 1.1
+ Python libraries like OpenCV, PySimpleGUI, PIL, scipy==1.1.0, dominate.

## How to use RMT
This Tutorial will show you how to construct metamorphic testing using UMT for Rule 7: Changing the driving scenes to rainy day will cause a deceleration by *t<sub>1</sub>%* to *t<sub>2</sub>%*.

First, you need to download the `model_steer.zip` from google drive and extract three models in it into the `./model` folder.
```https://drive.google.com/drive/u/1/folders/10xmtotVkSyFtwtCegmzscfQCabViJLbZ```

Then for the dataset, we provided a small set in RMT, and we will use it in this tutorial. If you want to use the full set, please refer to the section **dataset**.

To run RMT, you need to make sure that you have set up an environment same with the section **Prerequisites**.

Then you need to get into the `./UI` the folder in the terminal
```python
cd ./UI
```

In the `./UI` folder run the following code. Currently, this GUI has been tested on **Windows** and **Linux**. 
```python
python RMT_UI.py 
```

![Framework GUI](asset/rmt_ui_new_1.png)

In this interface, in the Transformation setting field, please select `ChangeScene` for Transformation and `Day2rain` for Weather. Then please select `X_N2` to apply transformation and click 'Add the transformation'. Next, in Mrsetting field select `A2D2` for Input dataset, `VGG16(A2D2)` for Model, and `0.1<((X_N1-X_N2)/X_N1)` for Higher order function. Finally, please click the `Generate test` button. After a few seconds, you can get the result of the metamorphic testing.

<!-- ***这个地方要加一个对各个Rule的Higher order function*** -->

![Framework testing result](asset/rmt_result.png)

## Overview and usage of the framework

![Workflow of RMT framework](asset/rmt_overview_2.png)

The framework takes a *rule* as the input. Based on the input, the framework would find the corresponding generator via `rmt.xml` that would be introduced later and call the generator to generate test sets. Then the framework will test driving models by test sets. We implemented the framework with a GUI.

The user could select rules defined in our paper, the threshold, and the model that needs to test. The framework could automatically select the correorganized sponding generator. Then the framework would generate test sets and the GUI would display a pair of sample images (source image and follow-up image). Finally, the GUI would show the test result.

For higher order function for each rule, we has few recommandation:
In this paper, we express 7 Simple Rules (SR): 
- **Rule 1**: M(X_N) < 60
- **Rule 2**: M(X_N) < 30
- **Rule 3**: 30 < (M(XN1) - M(XN2))=(M(XN1)) < 30
- **Rule 4**: 40 < (M(XN1) - M(XN2))=(M(XN1)) < 50
- **Rule 5**: 60 < (M(XN1) - M(XN2))=(M(XN1)) < 70
- **Rule 6**: 0 < (M(XN1) - M(XN2))=(M(XN1)) < 10
- **Rule 7**: 10 < (M(XN1) - M(XN2))=(M(XN1)) < 20
- **Rule 8**: 0 < (M(XN1) - M(XN2))=(M(XN1)) < 45
- **Rule 9**: (M(XN1) - M(XN2)) < 0

![Framework GUI](asset/rmt_ui_new_1.png)
![Framework testing result](asset/rmt_result.png)

Also, users could change the configs of default rules or add new rules by clicking the 'Config' button. The change of rules is saved in the `rmt.xml`. After clicking the button, 'Save new generator', the new rule will be saved, and it will be added on the main interface.

![Framework configs](asset/configs.png)
![Framework newgenerator](asset/add_new_generator.png)

Furthermore, the Input_path and Output_path refers to the relative path of the input and output folder of each generator, for example `../source_datasets/original` and `../follow_up_datasets/night`. However, for Pix2Pix generators, because of the special requirement of its model, the Input_path refers to the root of its input folder, for example `../source_datasets`. The Output_path of Pix2Pix is the same with other generators, which is the output folder of itself, for example, `./follow_up_datasets/add_car`.
After the transforming step, two folders will be generated in the Output_path to save the source image and generated image.



<!-- ### RMT Configuration file

 `rmt.xml` is provided to binding generators and metamorphic rules , which is organized like following:

```xml
<config>
	<generator id="1">
		<model_name>Pix2pixHD2</model_name>
		<transformation>Adding</transformation>
		<object>a vehicle</object>
		<object_type>object</object_type>
		<location>In the front</location>
		<Pre_conditions>There is no coordinates conflict to add a vehicle</Pre_conditions>
		<model_path>../model</model_path>
		
		<input_path>../source_datasets</input_path>
		<output_path>../follow_up_datasets</output_path>
		<running_script>python ../generators/pix2pixHD-master/image_control.py --checkpoints_dir ../models --name label2city --add_object car --feature ../generators/pix2pixHD-master/car.npy</running_script>
	</generator>
</config>
```

Based on the configuration file, the framework could know how to run the generator by the configuration file. The `rmt.xml` of this project is at the root path. -->

### Pretrained models and datasets

The related dataset and pretrained models are saved in the Google Drive Folder:

```https://drive.google.com/drive/u/1/folders/10xmtotVkSyFtwtCegmzscfQCabViJLbZ```

***重新传一下A2D2和新模型***

## Dataset

In experiments we used *A2D2* dataset, which is published by Audi and contains images extracted from driving videos. It provides many useful labels including class level and instance level semantic labels. Driving scenes vary from urban to country scenes. We could thus apply all rules on this dataset. We used OpenCV to add objects for Rules 1-5 and Rules 8- 9. Similar with Cityscapes, we trained UGATIT for Rules 6-7 and not all frames in the test set are applicable for each metamorphic transformation.

  + Firstly, we use the dataset to train 3 autonomous driving E2E models with different CNN architectures for speed prediction. The inputs are driving scene images and the labels are speeds for each image provided in the dataset.
  + Secondly, we use the dataset to train transformations like Pix2pixHD GAN, which will be introduced in detail later.

For human evaluation, we used *Cityscape* dataset for rule 3-7, which contains real-world driving road images with cruising speed labels. Because the data is collected from the urban area and speed labels are in general lower than 45 km/h, the dataset is not suitable for applying Rule 1, 8. Also, urban driving scenes are too crowded to add objects both 30 and 50 meters in front of the main vehicle on the same source images as required by Rule 9, thus Rule 9 is not applied on Cityscapes either.

We also use a part of images from BDD100K dataset to train UNIT GAN for transforming day-time driving scenes to night time. We manually random select day-time and night time images to construct the dataset. The detailed training process of UNIT could be seen in the [official github](https://github.com/mingyuliutw/UNIT).

***A2D2的下载和配置***

The cityscape dataset could be downloaded from ```https://www.cityscapes-dataset.com/```. Then the dataset should be re-organized as [pix2pixHD official github](https://github.com/NVIDIA/pix2pixHD) introduces. Before you use the dataset for `UNIT`, `OpenCV` and `UGATIT`, please run the following code to change the image into 224\*224. Please put the source dataset and the formatted dataset in the same root folder. If you use this dataset for `Pix2pix`, you do not need to make changes on this dataset.

```python
python image_crop.py --input_path <> --output_path <>
```

<!-- + In experiments we used Cityscape dataset that is originally used as image segmentation dataset. We choose this dataset because it provides semantic labels for images, which are used by Pix2pix GAN to generate driving scene images and adding objects on images. In the dataset, the training set (2975 frames) and validation set (500) frames have fine-grained semantic labels so we use them as the training set and testing set in our experiments. The dataset is used in two sub-tasks in our experiments.

  + Firstly, we use the dataset to train 3 autonomous driving E2E models with different CNN architectures for speed prediction. The inputs are driving scene images and the labels are speeds for each image provided in the dataset.
  + Secondly, we use the dataset to train Pix2pixHD GAN, which will be introduced in detail later.
+ We use a part of images from BDD100K dataset to train UNIT GAN for transforming day-time driving scenes to night time. We manually random select day-time and night time images to construct the dataset. The detailed training process of UNIT could be seen in the [official github](https://github.com/mingyuliutw/UNIT).

+ The cityscape dataset could be downloaded from ```https://www.cityscapes-dataset.com/```. Then the dataset should be re-organized as [pix2pixHD official github](https://github.com/NVIDIA/pix2pixHD) introduces. Before you use the dataset for `UNIT`, `OpenCV` and `UGATIT`, please run the following code to change the image into 224\*224. Please put the source dataset and the formatted dataset in the same root folder. If you use this dataset for `Pix2pix`, you do not need to make changes on this dataset. -->




## Autonomous driving E2E model training

In our experiments, we trained three different CNNs named Epoch (BaseCNN in code), VGG16, and Resnet101. Epoch is an architecture proposed in [Udacity Challenge 2](https://github.com/udacity/self-driving-car/blob/master/steering-models/community-models/cg23/epoch_model.py), we implemented the same architecture in Pytorch. VGG16 and Resnet101 are two classic transfer learning networks. We replaced their last classification layers with linear regression layers. The details could be seen in the file `model.py`. 

The `Cityscapes` dataset could be downloaded from the official website. `leftImg8bit_trainvaltest.zip`, and `vehicle_trainvaltest.zip` should be downloaded, unzipped, and organized as the following architecture. `train` and `val` folders store driving images from `leftImg8bit_trainvaltest.zip`. `vehicle` folder contains files from  `vehicle_trainvaltest.zip`

.
To train a driving model, running the command

```python
python train.py --model_name <> --data_root <> 
```

`model_name` could be `epoch`, `vgg16`, or `resnet101`. `data_root` is the root path of your Cityscape dataset. Other optional parameters could be seen in the file `train.py`.

## Pix2pixHD training

### Pix2pix training

Pix2pixHD is a GAN proposed by Nvidia for image-to-image translation. In our experiments, we added functions on it to manipulate instances (adding vehicles, bicycles, and pedestrians). The configuration detail could be seen on the [official repository](https://github.com/NVIDIA/pix2pixHD) 

To train Pix2pixHD,  the Cityscapes dataset should be put into `datasets/cityscapes` directory following the instruction in the official repository.  Then set the dataset path in file `base_option.py`. Finally, Opening the  terminal in the folder `generators/pix2pixHD-master`  and running the command:

```python
python train.py --name label2city_512p --instance_feat
```

When the training is finished, running

```python
python encode_features.py
```

to generate learned features for each class at the instance level.

## Human evaluation

We invited six people who have sufficient driving experiences to evaluate our testing results. We provided them the source test set and follow-up test set. They ranked the seasonality of the prediction change.  Images for human evaluation could be downloaded from [here](https://drive.google.com/open?id=1k7YURrxI4wJ2qETzy1GDagYR1FRjG-wW). And the link to the result of the human evaluation is [here](https://drive.google.com/open?id=1gVtA44abvsdjikQnvEVkoSJvh8tawyhC).
